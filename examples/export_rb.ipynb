{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f269085a",
   "metadata": {},
   "source": [
    "# Export ReplayBuffer from LoCo-MuJoCo (UnitreeG1, default:walking)\n",
    "\n",
    "- Loads Unitree G1 in LoCo-MuJoCo and selects the 'walking' motion from the default dataset (alias: 'walk').\n",
    "- Uses `TrajectoryDatasetManager` to create Zarr if missing and to step reference data.\n",
    "- Builds a TorchRL memmap-backed replay buffer from the saved Zarr.\n",
    "- Demonstrates step-wise sequential sampling and random minibatch sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, json, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from tensordict import TensorDict\n",
    "\n",
    "# Repo import for local package\n",
    "repo_root = Path.cwd().parent\n",
    "if (repo_root / 'src').exists():\n",
    "    sys.path.insert(0, str(repo_root/ 'src'))\n",
    "\n",
    "from iltools_datasets.loco_mujoco.loader import LocoMuJoCoLoader\n",
    "from iltools_datasets.manager import TrajectoryDatasetManager\n",
    "from iltools_datasets.replay_export import build_replay_from_zarr\n",
    "from iltools_datasets.replay_manager import EnvAssignment\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fad519",
   "metadata": {},
   "source": [
    "## Configure LoCo-MuJoCo (G1 walking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3850f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoCo-MuJoCo base cfg (see tests for example usage)\n",
    "basic_cfg = DictConfig({\n",
    "    'dataset': { 'trajectories': { 'default': ['walk'], 'amass': [], 'lafan1': [] } },\n",
    "    'control_freq': 50.0,\n",
    "    'window_size': 4,\n",
    "    'sim': { 'dt': 0.001 },\n",
    "    'decimation': 20,\n",
    "})\n",
    "\n",
    "# Resolve joint names via loader metadata to satisfy manager mapping\n",
    "tmp_loader = LocoMuJoCoLoader(env_name='UnitreeG1', cfg=basic_cfg)\n",
    "joint_names = list(tmp_loader.metadata.joint_names)\n",
    "print('Found', len(joint_names), 'joints')\n",
    "print(\"joint_names\", joint_names)\n",
    "del tmp_loader\n",
    "joint_names = joint_names[1:] # no need for root joint\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(repo_root)/'examples'/ 'data'/ 'g1_default_walk'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ZARR_PATH = DATA_DIR/ 'trajectories.zarr'\n",
    "\n",
    "# Manager cfg (creates Zarr if missing using LocoMuJoCoLoader)\n",
    "mgr_cfg = DictConfig({\n",
    "    'dataset_path': str(DATA_DIR),\n",
    "    'dataset': { 'trajectories': { 'default': ['walk'], 'amass': [], 'lafan1': [] } },\n",
    "    'loader_type': 'loco_mujoco',\n",
    "    'loader_kwargs': { 'env_name': 'UnitreeG1', 'cfg': basic_cfg },\n",
    "    'assignment_strategy': 'sequential',\n",
    "    'window_size': 4,\n",
    "    'target_joint_names': joint_names,\n",
    "    'reference_joint_names': joint_names,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = TrajectoryDatasetManager(cfg=mgr_cfg, num_envs=8, device=\"cuda:0\")\n",
    "manager.reset_trajectories()\n",
    "ref0 = manager.get_reference_data()\n",
    "print(\"Reference data keys:\", list(ref0.keys()))\n",
    "print(\"Zarr ready at:\", ZARR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46cc3c",
   "metadata": {},
   "source": [
    "## Build memmap-backed replay buffer from Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ebb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export replay using qpos as observation; action auto-detected if present\n",
    "replay_mgr = build_replay_from_zarr(\n",
    "    zarr_path=str(ZARR_PATH),\n",
    "    scratch_dir=str(DATA_DIR/ 'memmap'),\n",
    "    obs_keys=['qpos'],\n",
    "    concat_obs_to_key='observation',\n",
    "    include_terminated=True, include_truncated=True,\n",
    ")\n",
    "print('Replay transitions available:', len(replay_mgr.buffer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = replay_mgr.buffer\n",
    "buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b99ce3",
   "metadata": {},
   "source": [
    "## Step-wise sequential sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple assignment: all 8 envs read the first segment (task 0, traj 0)\n",
    "asg = [EnvAssignment(task_id=0, traj_id=0, step=i) for i in range(8)]\n",
    "replay_mgr.set_assignment(asg)\n",
    "# Sample twice; each env advances one step and wraps per-trajectory\n",
    "b1 = replay_mgr.buffer.sample()\n",
    "b2 = replay_mgr.buffer.sample()\n",
    "print('Sequential batch1 shape:', {k: tuple(v.shape) for k,v in b1.items() if isinstance(v, torch.Tensor)})\n",
    "print('Sequential batch2 observation head:', b2['observation'][:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12992d04",
   "metadata": {},
   "source": [
    "## Random minibatch sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d82de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to uniform random minibatch sampler (without replacement)\n",
    "replay_mgr.set_uniform_sampler(batch_size=1024, without_replacement=True)\n",
    "rb_batch = replay_mgr.buffer.sample()\n",
    "print('Random minibatch size:', rb_batch.batch_size)\n",
    "print('Keys:', list(rb_batch.keys(True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e0d01",
   "metadata": {},
   "source": [
    "## Replay Buffer Examples and Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a579d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect segment metadata and presence of auxiliary keys\n",
    "print('Num segments:', len(replay_mgr.segments))\n",
    "print('First 3 segments:', [ (s.task_id, s.traj_id, s.length) for s in replay_mgr.segments[:3] ])\n",
    "batch = replay_mgr.buffer.sample()\n",
    "print('Sampled keys:', list(batch.keys(True)))\n",
    "print('Has terminated?', 'terminated' in batch, 'Has truncated?', 'truncated' in batch)\n",
    "print('Batch sizes:', {k: tuple(v.shape) for k,v in batch.items() if hasattr(v, 'shape')})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba57f35",
   "metadata": {},
   "source": [
    "### Step-wise Sequential Sampler (assignment and wraparound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86026f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all envs to the first segment and stagger start steps to show progression\n",
    "first_seg = replay_mgr.segments[0]\n",
    "num_envs = 6\n",
    "asg = [EnvAssignment(task_id=first_seg.task_id, traj_id=first_seg.traj_id, step=i) for i in range(num_envs)]\n",
    "replay_mgr.set_assignment(asg)\n",
    "b1 = replay_mgr.buffer.sample()\n",
    "b2 = replay_mgr.buffer.sample()\n",
    "print('Sequential sampler shapes:', b1['observation'].shape, b2['observation'].shape)\n",
    "print('First env obs head b1/b2:', b1['observation'][0, :4], b2['observation'][0, :4])\n",
    "# Note: Actual values depend on dataset; we demonstrate API and per-call advancement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f66b9d",
   "metadata": {},
   "source": [
    "### Random Minibatch Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e06d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to uniform minibatching (without replacement)\n",
    "replay_mgr.set_uniform_sampler(batch_size=512, without_replacement=True)\n",
    "rb_batch = replay_mgr.buffer.sample()\n",
    "print('Uniform minibatch size:', rb_batch.batch_size)\n",
    "# With replacement (sampler=None)\n",
    "replay_mgr.set_uniform_sampler(batch_size=128, without_replacement=False)\n",
    "rb_batch_rep = replay_mgr.buffer.sample()\n",
    "print('With-replacement minibatch size:', rb_batch_rep.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b85ed",
   "metadata": {},
   "source": [
    "### Device Transform (prefetch to GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of the replay_manager module to pick up latest changes\n",
    "import importlib\n",
    "import iltools_datasets.replay_manager\n",
    "importlib.reload(iltools_datasets.replay_manager)\n",
    "\n",
    "target_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Target device:', target_device)\n",
    "replay_mgr.set_device_transform(target_device)\n",
    "bs = replay_mgr.buffer.sample()\n",
    "print('Device:', bs['observation'].device)\n",
    "print('Device types match:', bs['observation'].device.type == target_device.type)\n",
    "if bs['observation'].device.type == target_device.type:\n",
    "    print('✅ Device transform ok')\n",
    "else:\n",
    "    print('❌ Device transform failed - data still on CPU')\n",
    "    print('This might be due to buffer recreation losing transforms. Check replay_manager.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a44b7",
   "metadata": {},
   "source": [
    "### Synthetic Tests (deterministic observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iltools_datasets.replay_manager import ExpertReplayManager, ExpertReplaySpec\n",
    "from iltools_datasets.replay_memmap import build_trajectory_td, Segment\n",
    "\n",
    "def _mk_traj(task_id: int, traj_id: int, T: int, obs_dim: int = 3, act_dim: int = 1):\n",
    "    t = torch.arange(T, dtype=torch.float32).unsqueeze(-1)\n",
    "    obs = torch.cat([torch.full_like(t, float(task_id)), torch.full_like(t, float(traj_id)), t], dim=1)\n",
    "    nxt = obs + 0.5\n",
    "    act = torch.zeros(T, act_dim)\n",
    "    return build_trajectory_td(observation=obs, action=act, next_observation=nxt)\n",
    "\n",
    "# Build small tasks set\n",
    "tasks = {0: [_mk_traj(0,0, T=3)], 1: [_mk_traj(1,0, T=2)]}\n",
    "tmp_dir = str((Path.cwd()/'_tmp_memmap').absolute())\n",
    "mgr2 = ExpertReplayManager(ExpertReplaySpec(tasks=tasks, scratch_dir=tmp_dir, device='cpu', sample_batch_size=4))\n",
    "\n",
    "# Sequential assignment for 3 envs\n",
    "asg = [EnvAssignment(0,0,0), EnvAssignment(1,0,0), EnvAssignment(0,0,2)]\n",
    "mgr2.set_assignment(asg)\n",
    "out = mgr2.buffer.sample()\n",
    "obs = out['observation']\n",
    "assert torch.allclose(obs[0], torch.tensor([0.0,0.0,0.0]))\n",
    "assert torch.allclose(obs[1], torch.tensor([1.0,0.0,0.0]))\n",
    "assert torch.allclose(obs[2], torch.tensor([0.0,0.0,2.0]))\n",
    "out2 = mgr2.buffer.sample()\n",
    "obs2 = out2['observation']\n",
    "assert torch.allclose(obs2[0], torch.tensor([0.0,0.0,1.0]))\n",
    "assert torch.allclose(obs2[1], torch.tensor([1.0,0.0,1.0]))\n",
    "assert torch.allclose(obs2[2], torch.tensor([0.0,0.0,0.0]))\n",
    "print('✅ Sequential sampler synthetic test passed')\n",
    "\n",
    "# Uniform samplers\n",
    "mgr2.set_uniform_sampler(batch_size=5, without_replacement=True)\n",
    "u1 = mgr2.buffer.sample(); assert u1.batch_size[0] == 5\n",
    "mgr2.set_uniform_sampler(batch_size=3, without_replacement=False)\n",
    "u2 = mgr2.buffer.sample(); assert u2.batch_size[0] == 3\n",
    "print('✅ Uniform sampler tests passed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc693307",
   "metadata": {},
   "source": [
    "### Zarr Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ea2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr, os\n",
    "if 'ZARR_PATH' not in globals():\n",
    "    print('ZARR_PATH is undefined. Run the configuration cells first.')\n",
    "else:\n",
    "    zp = str(ZARR_PATH)\n",
    "    if not os.path.exists(zp):\n",
    "        print('Zarr not found at', zp, '- run earlier cells to create it.')\n",
    "    else:\n",
    "        root = zarr.open_group(zp, mode='r')\n",
    "        print('Zarr:', zp)\n",
    "        for ds_name in getattr(root, 'group_keys', lambda: list(root.keys()))():\n",
    "            ds_group = root[ds_name]\n",
    "            try:\n",
    "                motions = list(ds_group.group_keys())\n",
    "            except Exception:\n",
    "                motions = [k for k in ds_group.keys() if isinstance(ds_group[k], zarr.hierarchy.Group)]\n",
    "            print(f'- Dataset source: {ds_name} (motions: {len(motions)})')\n",
    "            for motion in motions:\n",
    "                mg = ds_group[motion]\n",
    "                try:\n",
    "                    trajs = list(mg.group_keys())\n",
    "                except Exception:\n",
    "                    trajs = [k for k in mg.keys() if isinstance(mg[k], zarr.hierarchy.Group)]\n",
    "                lengths = []\n",
    "                for traj in trajs:\n",
    "                    tg = mg[traj]\n",
    "                    # Prefer 'qpos' to determine T, else first array key\n",
    "                    arr_key = 'qpos' if 'qpos' in tg else next((k for k in tg.keys() if isinstance(tg[k], zarr.core.Array)), None)\n",
    "                    T = int(tg[arr_key].shape[0]) if arr_key is not None else -1\n",
    "                    lengths.append(T)\n",
    "                total_T = sum(max(0, T) for T in lengths)\n",
    "                print(f'  • Motion: {motion:>20} | trajs: {len(trajs):3d} | mean T: { (sum(lengths)/len(lengths)) if lengths else 0:.1f} | total T: {total_T}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkillLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
